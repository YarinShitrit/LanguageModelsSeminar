{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Toward Optimal Retrieval: Dynamically Choosing `k` in Vector-Based Search\n",
    "\n",
    "\n",
    "## üìö Introduction to Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a hybrid architecture that combines the strengths of **retrieval-based** systems and **generative language models**. Instead of relying solely on a language model's internal parameters to answer questions or generate content, RAG explicitly augments the generation process by retrieving relevant external knowledge.\n",
    "\n",
    "The RAG architecture was introduced to address limitations in traditional language models, especially their tendency to **\"hallucinate\"** or produce incorrect information due to lack of grounding in external knowledge sources.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç Why Use RAG?\n",
    "\n",
    "Pre-trained language models (e.g., GPT, LLaMA, etc.) are trained on massive datasets but are inherently static:\n",
    "\n",
    "- They cannot learn new knowledge after training unless fine-tuned.\n",
    "- Their knowledge is limited to their training cutoff.\n",
    "- They struggle with domain-specific or long-tail queries.\n",
    "\n",
    "RAG solves these problems by:\n",
    "- **Fetching real-time or up-to-date information** from external sources like documents, databases, or knowledge graphs.\n",
    "- **Reducing hallucinations** by grounding responses in retrieved context.\n",
    "- **Improving performance** on specialized tasks or domains without retraining the model.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† How Does RAG Work?\n",
    "\n",
    "The RAG pipeline generally consists of **three main components**:\n",
    "\n",
    "1. **Encoder-based Retriever**\n",
    "   - Given an input query, a retriever fetches the top-*k* most relevant documents from a vectorDB (e.g., FAISS, ChromaDB) using vector similarity search.\n",
    "   - These documents serve as an external knowledge source.\n",
    "\n",
    "2. **Contextual Fusion**\n",
    "   - The retrieved documents are passed alongside the query into a language model (e.g., a transformer decoder) to generate an informed response.\n",
    "   - This can be done by concatenating the documents and query into a single prompt.\n",
    "\n",
    "3. **Generator**\n",
    "   - A generative model (like GPT or BART) produces the final output based on the augmented input (query + retrieved knowledge).\n",
    "\n",
    "```text\n",
    "User Query ‚Üí Retriever ‚Üí Top-k Documents ‚Üí Generator ‚Üí Final Answer\n"
   ],
   "id": "80ade0aed1be9747"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ‚ùó Problem - Limitations of Static `k` in Vector Similarity Search\n",
    "\n",
    "## Background\n",
    "\n",
    "In Retrieval-Augmented Generation (RAG) and other retrieval-based systems, **vector similarity search** is a key operation. Given a query, the system retrieves the top-*k* most similar documents from a vector database using semantic embeddings and similarity metrics (e.g., cosine similarity or dot product).\n",
    "\n",
    "The parameter **`k`** represents the number of documents to retrieve per query. In most RAG implementations, `k` is set to a **fixed value** (e.g., `k=3` or `k=5`) for all inputs.\n",
    "\n",
    "While simple and easy to implement, **using a static value of `k` across all inputs introduces several problems** that can negatively impact retrieval relevance, model performance, and computational efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç The Core Problem: One-Size-Does-Not-Fit-All\n",
    "\n",
    "Different user queries or prompts have different levels of complexity, ambiguity, and knowledge requirements. However, a static `k` assumes that **every query benefits equally from the same number of retrieved documents** ‚Äî which is often not true.\n",
    "\n",
    "---\n",
    "\n",
    "## üî• Why a Fixed `k` is Suboptimal\n",
    "\n",
    "### 1. **Under-retrieval (k too small)**\n",
    "- Important context may be **missed**, especially for complex or vague queries.\n",
    "- Language model generates **incomplete or hallucinated** responses due to lack of sufficient information.\n",
    "- Example: A legal or medical question might require 10+ documents to cover relevant information.\n",
    "\n",
    "### 2. **Over-retrieval (k too large)**\n",
    "- Irrelevant or noisy documents may dilute the useful context.\n",
    "- More documents ‚Üí longer input prompt ‚Üí higher **token costs** in LLMs.\n",
    "- May confuse the model, especially when irrelevant docs are included.\n",
    "- Wastes compute, memory, and latency for simple, narrow queries.\n",
    "\n",
    "### 3. **No Adaptivity to Query Entropy or Difficulty**\n",
    "- Not all queries are created equal:\n",
    "  - Some are **simple and factoid-like** (\"What is the capital of Italy?\")\n",
    "  - Others are **ambiguous, multi-faceted, or domain-specific**\n",
    "- Fixed `k` ignores this variability, leading to suboptimal results in either direction.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Research Objective\n",
    "\n",
    "The goal of this research is to **dynamically determine the optimal value of `k` per query**, based on characteristics of the input or the retrieval results ‚Äî such as:\n",
    "\n",
    "- Query entropy or uncertainty\n",
    "- Query length and type\n",
    "- Similarity distribution of top retrieved documents\n",
    "- Historical performance metrics\n",
    "\n",
    "By intelligently adjusting `k`, we aim to improve:\n",
    "\n",
    "- Retrieval relevance and precision\n",
    "- LLM answer quality\n",
    "- Efficiency and cost-effectiveness of the system\n",
    "\n",
    "In the next sections, we will explore strategies, algorithms, and evaluation methods for achieving this dynamic retrieval objective.\n",
    "\n"
   ],
   "id": "18b88198ecf7fadb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üß™ Approaches to Dynamically Determine `k` in Vector Similarity Search\n",
    "\n",
    "Choosing the right number of documents (`k`) to retrieve for a given query is crucial for balancing relevance, efficiency, and downstream model performance in RAG systems.\n",
    "\n",
    "Here we explore three statistically-motivated techniques for determining `k` dynamically based on the **distribution of similarity scores** between the query and candidate documents.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Benjamini‚ÄìHochberg Procedure (False Discovery Rate Control)\n",
    "\n",
    "### üîç Overview\n",
    "The **Benjamini‚ÄìHochberg (BH) procedure** is a statistical method for **controlling the False Discovery Rate (FDR)** ‚Äî the expected proportion of false positives among the selected items. It is typically used in multiple hypothesis testing scenarios.\n",
    "\n",
    "In the context of vector similarity search:\n",
    "- Each document can be treated as a \"hypothesis\" (i.e., \"Is this document relevant?\").\n",
    "- We compute **p-values** (or a proxy derived from similarity scores) for each document.\n",
    "- BH controls the expected rate of false positives among the selected top-*k* documents.\n",
    "\n",
    "### ‚öôÔ∏è How it works\n",
    "1. Convert similarity scores into pseudo p-values (e.g., via ranking or null distribution assumptions).\n",
    "2. Sort these p-values in ascending order: $( p_1, p_2, ..., p_n )$\n",
    "3. For each p-value, check:\n",
    "$\n",
    "p_i \\leq \\frac{i}{n} \\cdot \\alpha\n",
    "$\n",
    "\n",
    "\n",
    "   where $\\alpha \\$ is the desired FDR (e.g., 0.05).\n",
    "4. Select the **largest `i`** that satisfies the inequality ‚Äî set `k = i`.\n",
    "\n",
    "### ‚úÖ Pros\n",
    "- More power than conservative tests like Bonferroni.\n",
    "- Controls false discovery rather than per-test error.\n",
    "- Adapts naturally to the number and quality of candidate documents.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonferroni Correction (Family-Wise Error Rate Control)\n",
    "\n",
    "### üîç Overview\n",
    "The **Bonferroni correction** is a conservative method to control the **Family-Wise Error Rate (FWER)** ‚Äî the probability of making **any** false discovery.\n",
    "\n",
    "It is stricter than BH and is often used when **false positives must be avoided at all costs**.\n",
    "\n",
    "### ‚öôÔ∏è How it works\n",
    "1. Convert similarity scores into pseudo p-values.\n",
    "2. Adjust the threshold using:\n",
    "   $\n",
    "   \\alpha' = \\frac{\\alpha}{n}\n",
    "   $\n",
    "   where $ n $ is the total number of documents and $\\alpha$ is the desired overall error rate (e.g., 0.05).\n",
    "3. Select all documents with $p_i \\leq \\alpha'$\n",
    "4. Set `k` as the number of documents that meet the criterion.\n",
    "\n",
    "### ‚ö†Ô∏è Caveats\n",
    "- Very conservative: often results in **low `k`** or even `k=0`, especially when many candidates are noisy or weakly similar.\n",
    "- Better suited for high-stakes applications where **false positives are expensive**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Higher Criticism Thresholding\n",
    "\n",
    "### üîç Overview\n",
    "**Higher Criticism (HC)** is a powerful method for detecting **sparse and weak signals** in large-scale testing problems. It is especially effective when:\n",
    "- Only a small fraction of documents are truly relevant.\n",
    "- Their similarity scores are only **slightly stronger than noise**.\n",
    "\n",
    "Originally proposed by Donoho & Jin (2008), HC finds an **optimal threshold** by balancing signal detection and noise suppression.\n",
    "\n",
    "üìÑ [PNAS Article](https://www.pnas.org/doi/abs/10.1073/pnas.0807471105)\n",
    "\n",
    "### ‚öôÔ∏è How it works\n",
    "1. Convert similarity scores into z-scores or p-values under a null model.\n",
    "2. Compute the **Higher Criticism statistic** for each ordered p-value:\n",
    "   $\n",
    "   HC(i) = \\frac{\\sqrt{n} \\left( \\frac{i}{n} - p_i \\right)}{\\sqrt{p_i (1 - p_i)}}\n",
    "   $\n",
    "3. Find the **index `i` with the maximum HC value** ‚Üí this index indicates the optimal threshold.\n",
    "4. Set `k = i`, selecting the top-`k` documents as relevant.\n",
    "\n",
    "### üöÄ Advantages\n",
    "- Adaptive to sparse signal settings.\n",
    "- Theoretically powerful under weak signal conditions.\n",
    "- Can outperform both BH and Bonferroni when only a few strong matches exist.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Summary of Methods\n",
    "\n",
    "| Method                    | Controls           | Conservative? | Adaptive? | Best Use Case                                  |\n",
    "|--------------------------|--------------------|---------------|-----------|------------------------------------------------|\n",
    "| Benjamini-Hochberg (BH)  | False Discovery Rate (FDR) | ‚ùå No         | ‚úÖ Yes    | General-purpose; balances false positives      |\n",
    "| Bonferroni Correction     | Family-Wise Error Rate (FWER) | ‚úÖ Yes        | ‚ùå No     | High-stakes tasks; requires strict precision   |\n",
    "| Higher Criticism (HC)    | Sparse Signal Detection | ‚ö† Depends     | ‚úÖ Yes    | Sparse, noisy data; detecting weak signals     |\n",
    "\n",
    "---\n",
    "\n"
   ],
   "id": "3ad63adeddb5bc47"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
