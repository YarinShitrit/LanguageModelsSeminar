{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Toward Optimal Retrieval: Dynamically Choosing `k` in Vector-Based Search\n",
    "\n",
    "\n",
    "## ðŸ“š Introduction to Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a hybrid architecture that combines the strengths of **retrieval-based** systems and **generative language models**. Instead of relying solely on a language model's internal parameters to answer questions or generate content, RAG explicitly augments the generation process by retrieving relevant external knowledge.\n",
    "\n",
    "The RAG architecture was introduced to address limitations in traditional language models, especially their tendency to **\"hallucinate\"** or produce incorrect information due to lack of grounding in external knowledge sources.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” Why Use RAG?\n",
    "\n",
    "Pre-trained language models (e.g., GPT, LLaMA, etc.) are trained on massive datasets but are inherently static:\n",
    "\n",
    "- They cannot learn new knowledge after training unless fine-tuned.\n",
    "- Their knowledge is limited to their training cutoff.\n",
    "- They struggle with domain-specific or long-tail queries.\n",
    "\n",
    "RAG solves these problems by:\n",
    "- **Fetching real-time or up-to-date information** from external sources like documents, databases, or knowledge graphs.\n",
    "- **Reducing hallucinations** by grounding responses in retrieved context.\n",
    "- **Improving performance** on specialized tasks or domains without retraining the model.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  How Does RAG Work?\n",
    "\n",
    "The RAG pipeline generally consists of **three main components**:\n",
    "\n",
    "1. **Encoder-based Retriever**\n",
    "   - Given an input query, a retriever fetches the top-*k* most relevant documents from a vectorDB (e.g., FAISS, ChromaDB) using vector similarity search.\n",
    "   - These documents serve as an external knowledge source.\n",
    "\n",
    "2. **Contextual Fusion**\n",
    "   - The retrieved documents are passed alongside the query into a language model (e.g., a transformer decoder) to generate an informed response.\n",
    "   - This can be done by concatenating the documents and query into a single prompt.\n",
    "\n",
    "3. **Generator**\n",
    "   - A generative model (like GPT or BART) produces the final output based on the augmented input (query + retrieved knowledge).\n",
    "\n",
    "```text\n",
    "User Query â†’ Retriever â†’ Top-k Documents â†’ Generator â†’ Final Answer\n"
   ],
   "id": "80ade0aed1be9747"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# â— Problem - Limitations of Static `k` in Vector Similarity Search\n",
    "\n",
    "## Background\n",
    "\n",
    "In Retrieval-Augmented Generation (RAG) and other retrieval-based systems, **vector similarity search** is a key operation. Given a query, the system retrieves the top-*k* most similar documents from a vector database using semantic embeddings and similarity metrics (e.g., cosine similarity or dot product).\n",
    "\n",
    "The parameter **`k`** represents the number of documents to retrieve per query. In most RAG implementations, `k` is set to a **fixed value** (e.g., `k=3` or `k=5`) for all inputs.\n",
    "\n",
    "While simple and easy to implement, **using a static value of `k` across all inputs introduces several problems** that can negatively impact retrieval relevance, model performance, and computational efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” The Core Problem: One-Size-Does-Not-Fit-All\n",
    "\n",
    "Different user queries or prompts have different levels of complexity, ambiguity, and knowledge requirements. However, a static `k` assumes that **every query benefits equally from the same number of retrieved documents** â€” which is often not true.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¥ Why a Fixed `k` is Suboptimal\n",
    "\n",
    "### 1. **Under-retrieval (k too small)**\n",
    "- Important context may be **missed**, especially for complex or vague queries.\n",
    "- Language model generates **incomplete or hallucinated** responses due to lack of sufficient information.\n",
    "- Example: A legal or medical question might require 10+ documents to cover relevant information.\n",
    "\n",
    "### 2. **Over-retrieval (k too large)**\n",
    "- Irrelevant or noisy documents may dilute the useful context.\n",
    "- More documents â†’ longer input prompt â†’ higher **token costs** in LLMs.\n",
    "- May confuse the model, especially when irrelevant docs are included.\n",
    "- Wastes compute, memory, and latency for simple, narrow queries.\n",
    "\n",
    "### 3. **No Adaptivity to Query Entropy or Difficulty**\n",
    "- Not all queries are created equal:\n",
    "  - Some are **simple and factoid-like** (\"What is the capital of Italy?\")\n",
    "  - Others are **ambiguous, multi-faceted, or domain-specific**\n",
    "- Fixed `k` ignores this variability, leading to suboptimal results in either direction.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Research Objective\n",
    "\n",
    "The goal of this research is to **dynamically determine the optimal value of `k` per query**, based on characteristics of the input or the retrieval results â€” such as:\n",
    "\n",
    "- Query entropy or uncertainty\n",
    "- Query length and type\n",
    "- Similarity distribution of top retrieved documents\n",
    "- Historical performance metrics\n",
    "\n",
    "By intelligently adjusting `k`, we aim to improve:\n",
    "\n",
    "- Retrieval relevance and precision\n",
    "- LLM answer quality\n",
    "- Efficiency and cost-effectiveness of the system\n",
    "\n",
    "In the next sections, we will explore strategies, algorithms, and evaluation methods for achieving this dynamic retrieval objective.\n",
    "\n"
   ],
   "id": "18b88198ecf7fadb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ðŸ§ª Approaches to Dynamically Determine `k` in Vector Similarity Search\n",
    "\n",
    "Choosing the right number of documents (`k`) to retrieve for a given query is crucial for balancing relevance, efficiency, and downstream model performance in RAG systems.\n",
    "\n",
    "Here we explore three statistically-motivated techniques for determining `k` dynamically based on the **distribution of similarity scores** between the query and candidate documents.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Benjaminiâ€“Hochberg Procedure (False Discovery Rate Control)\n",
    "\n",
    "### ðŸ” Overview\n",
    "The **Benjaminiâ€“Hochberg (BH) procedure** is a statistical method for **controlling the False Discovery Rate (FDR)** â€” the expected proportion of false positives among the selected items. It is typically used in multiple hypothesis testing scenarios.\n",
    "\n",
    "In the context of vector similarity search:\n",
    "- Each document can be treated as a \"hypothesis\" (i.e., \"Is this document relevant?\").\n",
    "- We compute **p-values** (or a proxy derived from similarity scores) for each document.\n",
    "- BH controls the expected rate of false positives among the selected top-*k* documents.\n",
    "\n",
    "### âš™ï¸ How it works\n",
    "1. Convert similarity scores into pseudo p-values (e.g., via ranking or null distribution assumptions).\n",
    "2. Sort these p-values in ascending order: $( p_1, p_2, ..., p_n )$\n",
    "3. For each p-value, check:\n",
    "$\n",
    "p_i \\leq \\frac{i}{n} \\cdot \\alpha\n",
    "$\n",
    "\n",
    "\n",
    "   where $\\alpha \\$ is the desired FDR (e.g., 0.05).\n",
    "4. Select the **largest `i`** that satisfies the inequality â€” set `k = i`.\n",
    "\n",
    "### âœ… Pros\n",
    "- More power than conservative tests like Bonferroni.\n",
    "- Controls false discovery rather than per-test error.\n",
    "- Adapts naturally to the number and quality of candidate documents.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonferroni Correction (Family-Wise Error Rate Control)\n",
    "\n",
    "### ðŸ” Overview\n",
    "The **Bonferroni correction** is a conservative method to control the **Family-Wise Error Rate (FWER)** â€” the probability of making **any** false discovery.\n",
    "\n",
    "It is stricter than BH and is often used when **false positives must be avoided at all costs**.\n",
    "\n",
    "### âš™ï¸ How it works\n",
    "1. Convert similarity scores into pseudo p-values.\n",
    "2. Adjust the threshold using:\n",
    "   $\n",
    "   \\alpha' = \\frac{\\alpha}{n}\n",
    "   $\n",
    "   where $ n $ is the total number of documents and $\\alpha$ is the desired overall error rate (e.g., 0.05).\n",
    "3. Select all documents with $p_i \\leq \\alpha'$\n",
    "4. Set `k` as the number of documents that meet the criterion.\n",
    "\n",
    "### âš ï¸ Caveats\n",
    "- Very conservative: often results in **low `k`** or even `k=0`, especially when many candidates are noisy or weakly similar.\n",
    "- Better suited for high-stakes applications where **false positives are expensive**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Higher Criticism Thresholding\n",
    "\n",
    "### ðŸ” Overview\n",
    "**Higher Criticism (HC)** is a powerful method for detecting **sparse and weak signals** in large-scale testing problems. It is especially effective when:\n",
    "- Only a small fraction of documents are truly relevant.\n",
    "- Their similarity scores are only **slightly stronger than noise**.\n",
    "\n",
    "Originally proposed by Donoho & Jin (2008), HC finds an **optimal threshold** by balancing signal detection and noise suppression.\n",
    "\n",
    "ðŸ“„ [PNAS Article](https://www.pnas.org/doi/abs/10.1073/pnas.0807471105)\n",
    "\n",
    "### âš™ï¸ How it works\n",
    "1. Convert similarity scores into z-scores or p-values under a null model.\n",
    "2. Compute the **Higher Criticism statistic** for each ordered p-value:\n",
    "   $\n",
    "   HC(i) = \\frac{\\sqrt{n} \\left( \\frac{i}{n} - p_i \\right)}{\\sqrt{p_i (1 - p_i)}}\n",
    "   $\n",
    "3. Find the **index `i` with the maximum HC value** â†’ this index indicates the optimal threshold.\n",
    "4. Set `k = i`, selecting the top-`k` documents as relevant.\n",
    "\n",
    "### ðŸš€ Advantages\n",
    "- Adaptive to sparse signal settings.\n",
    "- Theoretically powerful under weak signal conditions.\n",
    "- Can outperform both BH and Bonferroni when only a few strong matches exist.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Summary of Methods\n",
    "\n",
    "| Method                    | Controls           | Conservative? | Adaptive? | Best Use Case                                  |\n",
    "|--------------------------|--------------------|---------------|-----------|------------------------------------------------|\n",
    "| Benjamini-Hochberg (BH)  | False Discovery Rate (FDR) | âŒ No         | âœ… Yes    | General-purpose; balances false positives      |\n",
    "| Bonferroni Correction     | Family-Wise Error Rate (FWER) | âœ… Yes        | âŒ No     | High-stakes tasks; requires strict precision   |\n",
    "| Higher Criticism (HC)    | Sparse Signal Detection | âš  Depends     | âœ… Yes    | Sparse, noisy data; detecting weak signals     |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ðŸ“¦ Bonferroni Thresholding with Empirical Null Distribution\n",
    "\n",
    "This code demonstrates how to dynamically determine the optimal number of documents `k` to retrieve for a RAG system query, using **Bonferroni correction**. We generate an empirical null distribution from random query-document pairs sampled from our own RAG corpus to ensure realistic background noise modeling.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”§ Setup & Model Initialization\n",
    "We start by loading a sentence embedding model and defining our document corpus.\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Your RAG corpus (use real preprocessed passages)\n",
    "rag_corpus = [\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"Cats are independent animals.\",\n",
    "    \"The Eiffel Tower is a famous landmark in Paris.\",\n",
    "    \"Quantum mechanics explains the behavior of particles.\",\n",
    "    \"Vitamin C helps boost the immune system.\"\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Step 1: Create the Null Distribution\n",
    "We generate an empirical null distribution by computing similarity scores between random queries and randomly selected unrelated documents from your RAG corpus.\n",
    "\n",
    "```python\n",
    "def generate_null_distribution(random_queries, corpus, model, num_pairs=10000):\n",
    "    print(f\"Generating null distribution with {num_pairs} random query-doc pairs...\")\n",
    "    null_similarities = []\n",
    "\n",
    "    for _ in tqdm(range(num_pairs)):\n",
    "        q = random.choice(random_queries)\n",
    "        d = random.choice(corpus)\n",
    "\n",
    "        q_embed = model.encode(q, convert_to_numpy=True, normalize_embeddings=True)\n",
    "        d_embed = model.encode(d, convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "        sim = np.dot(q_embed, d_embed)  # cosine similarity\n",
    "        null_similarities.append(sim)\n",
    "\n",
    "    return np.array(null_similarities)\n",
    "\n",
    "# Random unrelated queries\n",
    "random_queries = [\n",
    "    \"How do I reset my password?\",\n",
    "    \"What is the weather like in July?\",\n",
    "    \"Tips for learning Japanese online\",\n",
    "    \"What are the symptoms of flu?\",\n",
    "    \"How to cook rice perfectly?\"\n",
    "]\n",
    "\n",
    "# Generate the null distribution\n",
    "null_distribution = generate_null_distribution(random_queries, rag_corpus, model, num_pairs=10000)\n",
    "\n",
    "# Save for reuse\n",
    "np.save(\"null_distribution.npy\", null_distribution)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Step 2: Bonferroni Thresholding Function\n",
    "This function takes in similarity scores and the null distribution to calculate p-values and applies Bonferroni correction.\n",
    "\n",
    "```python\n",
    "def bonferroni_thresholding(similarities, null_distribution, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Apply Bonferroni correction to dynamically select top-k documents.\n",
    "    \"\"\"\n",
    "    m = len(similarities)\n",
    "    p_values = np.array([\n",
    "        np.mean(null_distribution >= s) for s in similarities\n",
    "    ])\n",
    "    threshold = alpha / m\n",
    "    selected_indices = np.where(p_values <= threshold)[0].tolist()\n",
    "    return selected_indices, p_values\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Step 3: Query Evaluation and Dynamic `k`\n",
    "This part simulates how youâ€™d retrieve documents and apply Bonferroni to determine the appropriate number of documents to include.\n",
    "\n",
    "```python\n",
    "# Example user query\n",
    "query = \"Tell me about France.\"\n",
    "query_embedding = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "# Compute similarities to all documents in corpus\n",
    "doc_embeddings = model.encode(rag_corpus, convert_to_numpy=True, normalize_embeddings=True)\n",
    "similarities = np.dot(doc_embeddings, query_embedding[0])\n",
    "\n",
    "# Apply Bonferroni thresholding\n",
    "selected_indices, p_vals = bonferroni_thresholding(similarities, null_distribution, alpha=0.05)\n",
    "\n",
    "# Display selected results\n",
    "print(f\"\\nQuery: {query}\\n\")\n",
    "print(f\"Selected top-{len(selected_indices)} documents using Bonferroni correction:\")\n",
    "for idx in selected_indices:\n",
    "    print(f\"[Score: {similarities[idx]:.4f} | p = {p_vals[idx]:.4f}] â†’ {rag_corpus[idx]}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Summary\n",
    "- This pipeline dynamically selects `k` documents based on statistical significance.\n",
    "- The **null distribution** reflects background similarity scores.\n",
    "- **Bonferroni correction** ensures a strict false positive control.\n",
    "\n",
    "âœ… Ideal for high-precision settings like legal, medical, or compliance-related RAG systems.\n",
    "\n"
   ],
   "id": "3ad63adeddb5bc47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T07:25:57.467827Z",
     "start_time": "2025-05-05T07:25:57.461672Z"
    }
   },
   "cell_type": "code",
   "source": "#todo add about put in cache all the embedding (vector index) and use ANN to find ",
   "id": "f81d3674d8d90156",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2f45203280ab36f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
